{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alphabet = {chr(i): i - 97 for i in range(97, 123)}\n",
    "alphabet[\"S\"] = 26; alphabet[\"E\"] = 27\n",
    "rev_alphabet = {v: k for k, v in alphabet.items()}\n",
    "\n",
    "def get_bigram_counts_with_delay(delay: int, names: list[torch.Tensor]):\n",
    "    counts = torch.zeros(len(alphabet), len(alphabet), dtype=torch.float)\n",
    "    for name in names:\n",
    "        for bigram in zip(name, name[delay:]):\n",
    "            counts[bigram[0], bigram[1]] += 1\n",
    "    counts = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return counts\n",
    "\n",
    "def build_dataset(max_delay: int):\n",
    "    names = [\"S\" * max_delay + name.strip().lower() + \"E\" for name in open(\"data/names.txt\").readlines()]\n",
    "    names = list(filter(lambda x: \"-\" not in x and \" \" not in x, names))\n",
    "\n",
    "    # Build dataset\n",
    "    names = [\"S\" + name.strip().lower() + \"E\" for name in open(\"data/names.txt\", \"r\").readlines()]\n",
    "    names = list(filter(lambda x: \"-\" not in x, names))\n",
    "\n",
    "    # Convert names to tensors\n",
    "    names = [torch.tensor([alphabet[char] for char in name]) for name in names]\n",
    "    \n",
    "    # Create the counts matrix for each delay\n",
    "    counts_per_delay = [get_bigram_counts_with_delay(delay, names) for delay in range(1, max_delay + 1)]\n",
    "\n",
    "    return names, counts_per_delay\n",
    "\n",
    "def display_name(name: list[int]):\n",
    "    return \"\".join([rev_alphabet[i] for i in name]).replace(\"S\", \"\").replace(\"E\", \"\")\n",
    "\n",
    "bigram_names, bigram_counts = build_dataset(1)\n",
    "delay_names, delay_counts = build_dataset(3)\n",
    "all_names = bigram_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models: Bigram, Delay, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A series of models that give a probability distribution over the next character\n",
    "from functools import reduce\n",
    "from torch import nn\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "\n",
    "    def forward(self, name: list[int]):\n",
    "        probs = self.counts[0][name[-1], :]\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n",
    "\n",
    "class DelayModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "    \n",
    "    def forward(self, name: list[int]):\n",
    "        rows = [\n",
    "            self.counts[i][name[-(i + 1)], :]\n",
    "            for i in range(len(self.counts))\n",
    "        ]\n",
    "\n",
    "        probs = reduce(lambda a, b: a * b, rows)\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n",
    "\n",
    "class NNDelayModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "    \n",
    "    def forward(self, name: list[int]):\n",
    "        rows = [\n",
    "            self.counts[i][name[-(i + 1)], :]\n",
    "            for i in range(len(self.counts))\n",
    "        ]\n",
    "\n",
    "        probs = reduce(lambda a, b: a * b, rows)\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a model, do some testing of it\n",
    "def generate_name(model: torch.nn.Module, initial: list[int], parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    name = [*initial] # Make a copy, otherwise you're updating the actual object, which is passed by pointer\n",
    "    i = 0\n",
    "    probs = ''\n",
    "    while name[-1] != alphabet[\"E\"] and i < 25:\n",
    "        probs = parse_output(model(prep_input(name)))\n",
    "        next_letter_idx = torch.multinomial(probs, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "        i += 1\n",
    "\n",
    "    return name[1:-1]\n",
    "\n",
    "def name_nll(model: torch.nn.Module, name: list[int], s_buffer: list[int], parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    nll = 0\n",
    "    name += [alphabet[\"E\"]]\n",
    "    full_name = s_buffer + name\n",
    "    s_buffer_len = len(full_name) - len(name)\n",
    "    for i in range(len(name)):\n",
    "        probs = parse_output(model(prep_input(full_name[:s_buffer_len + i])))\n",
    "        if not torch.sum(probs).isclose(torch.ones(())):\n",
    "            raise Exception(\"ERROR probs don't add to one: \" + str(torch.sum(probs).item()))\n",
    "        nll += -torch.log(0.00001 + probs[name[i]])\n",
    "\n",
    "    return nll.item()\n",
    "\n",
    "def test_model(model, prefix=[alphabet[\"S\"]], iters=100, parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    for _ in range(iters):\n",
    "        name = generate_name(model, prefix, parse_output=parse_output, prep_input=prep_input)\n",
    "        print(display_name(name))\n",
    "    \n",
    "    total = 0\n",
    "    count = 0\n",
    "    for name in bigram_names:\n",
    "        nll = name_nll(model, name, prefix, parse_output=parse_output, prep_input=prep_input)\n",
    "        total += nll\n",
    "        count += 1\n",
    "\n",
    "    print(\"Average NLL: \", total / count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chaualdiarerotritt\n",
      "hen\n",
      "rk\n",
      "sorckwane\n",
      "dommne\n",
      "sinodeaustilezerant\n",
      "hararioynetitry\n",
      "llettawerymudolffotoso\n",
      "aus\n",
      "jomo\n",
      "gosimariarn\n",
      "arerl\n",
      "ng\n",
      "udydell\n",
      "bjabelem\n",
      "wewob\n",
      "jel\n",
      "wonilerduian\n",
      "ugeiepaundanend\n",
      "th\n",
      "tthralonondeocyrthorky\n",
      "wanatorthud\n",
      "s\n",
      "jes\n",
      "lin\n",
      "an\n",
      "je\n",
      "in\n",
      "har\n",
      "ge\n",
      "rarerlain\n",
      "tern\n",
      "ti\n",
      "tckamilivan\n",
      "darde\n",
      "t\n",
      "jesthadrhuibbeannatanal\n",
      "cohon\n",
      "arswewior\n",
      "e\n",
      "usa\n",
      "s\n",
      "thorpainianoilel\n",
      "honnaleweynnicangun\n",
      "chan\n",
      "jas\n",
      "wiarurodewifontarnedomar\n",
      "zemameelematez\n",
      "o\n",
      "g\n",
      "pamo\n",
      "rapeenwawtuce\n",
      "y\n",
      "shewod\n",
      "hoh\n",
      "le\n",
      "hearolarl\n",
      "rr\n",
      "wellvi\n",
      "lalelb\n",
      "mobieunan\n",
      "ronint\n",
      "tod\n",
      "tteydenintiendeshmos\n",
      "ph\n",
      "m\n",
      "ch\n",
      "gurt\n",
      "arloher\n",
      "berburafror\n",
      "qun\n",
      "dusisho\n",
      "x\n",
      "nnierigus\n",
      "righarngerdys\n",
      "rido\n",
      "hite\n",
      "r\n",
      "shry\n",
      "henil\n",
      "keson\n",
      "tianusin\n",
      "kin\n",
      "ky\n",
      "hadmmallo\n",
      "lontarillvi\n",
      "ahild\n",
      "shahery\n",
      "eshbachelad\n",
      "lllyl\n",
      "hareisshan\n",
      "aho\n",
      "to\n",
      "kendorey\n",
      "feil\n",
      "n\n",
      "nl\n",
      "arduefo\n",
      "chaclied\n",
      "tare\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'Tensor' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [157], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bigram_model \u001b[39m=\u001b[39m BigramModel(bigram_counts)\n\u001b[0;32m----> 2\u001b[0m test_model(bigram_model, iters\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [156], line 35\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, prefix, iters, parse_output, prep_input)\u001b[0m\n\u001b[1;32m     33\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m bigram_names:\n\u001b[0;32m---> 35\u001b[0m     nll \u001b[39m=\u001b[39m name_nll(model, name, prefix, parse_output\u001b[39m=\u001b[39;49mparse_output, prep_input\u001b[39m=\u001b[39;49mprep_input)\n\u001b[1;32m     36\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nll\n\u001b[1;32m     37\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [156], line 16\u001b[0m, in \u001b[0;36mname_nll\u001b[0;34m(model, name, s_buffer, parse_output, prep_input)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mname_nll\u001b[39m(model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, name: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], s_buffer: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m], parse_output\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x, prep_input\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x):\n\u001b[1;32m     15\u001b[0m     nll \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     name \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [alphabet[\u001b[39m\"\u001b[39m\u001b[39mE\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m     17\u001b[0m     full_name \u001b[39m=\u001b[39m s_buffer \u001b[39m+\u001b[39m name\n\u001b[1;32m     18\u001b[0m     s_buffer_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(full_name) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(name)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'Tensor' and 'list'"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramModel(bigram_counts)\n",
    "test_model(bigram_model, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord\n",
      "ero\n",
      "air\n",
      "terie\n",
      "cen\n",
      "ole\n",
      "ner\n",
      "re\n",
      "den\n",
      "irmere\n",
      "are\n",
      "ale\n",
      "ore\n",
      "ain\n",
      "rian\n",
      "nee\n",
      "wil\n",
      "art\n",
      "rey\n",
      "aun\n",
      "amey\n",
      "ren\n",
      "ere\n",
      "aal\n",
      "ard\n",
      "lar\n",
      "rie\n",
      "aird\n",
      "arn\n",
      "ale\n",
      "rae\n",
      "ert\n",
      "raie\n",
      "larn\n",
      "ere\n",
      "enro\n",
      "rato\n",
      "eas\n",
      "rie\n",
      "mare\n",
      "set\n",
      "eate\n",
      "iel\n",
      "rel\n",
      "ereane\n",
      "aic\n",
      "esar\n",
      "rras\n",
      "ail\n",
      "are\n",
      "era\n",
      "arie\n",
      "san\n",
      "ier\n",
      "er\n",
      "rest\n",
      "rar\n",
      "arr\n",
      "ere\n",
      "len\n",
      "are\n",
      "arl\n",
      "ren\n",
      "aren\n",
      "her\n",
      "eld\n",
      "rin\n",
      "erd\n",
      "entan\n",
      "aro\n",
      "re\n",
      "ran\n",
      "rie\n",
      "arat\n",
      "are\n",
      "alle\n",
      "ari\n",
      "art\n",
      "arie\n",
      "are\n",
      "ari\n",
      "anarl\n",
      "rrt\n",
      "rel\n",
      "ren\n",
      "sre\n",
      "ric\n",
      "are\n",
      "all\n",
      "aite\n",
      "rie\n",
      "rain\n",
      "ren\n",
      "ere\n",
      "ral\n",
      "lan\n",
      "ari\n",
      "lan\n",
      "atle\n",
      "arder\n",
      "Average NLL:  29.5828208732605\n"
     ]
    }
   ],
   "source": [
    "delay_model = DelayModel(delay_counts)\n",
    "test_model(delay_model, prefix=3*[alphabet[\"S\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network - scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NLL:  149.0485184955597\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "def one_hot_encode_letter(letter: int):\n",
    "    one_hot = torch.zeros(len(alphabet))\n",
    "    one_hot[letter] = 1\n",
    "    return one_hot\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, param_size: int):\n",
    "        super().__init__()\n",
    "        self.U = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.W = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.V = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.h = torch.randn(param_size)\n",
    "        self.bias_h = nn.Parameter(torch.randn(param_size))\n",
    "        self.bias_o = nn.Parameter(torch.randn(param_size))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    # x should be a one-hot vector\n",
    "    def forward(self, x):\n",
    "        x = one_hot_encode_letter(x)\n",
    "        self.h = torch.tanh(self.U @ x + self.W @ self.h + self.bias_h)\n",
    "        return self.softmax(self.bias_o + self.V @ self.h)\n",
    "\n",
    "    def detach_h(self):\n",
    "        self.h = self.h.detach()\n",
    "\n",
    "rnn_model = RNN(len(alphabet))\n",
    "total = 0\n",
    "for i in range(iters):\n",
    "    name = generate_name(rnn_model, [alphabet[\"S\"]])\n",
    "    # print(display_name(name))\n",
    "    nll = name_nll(rnn_model, name, [alphabet[\"S\"]])\n",
    "    total += nll\n",
    "    # print(name_nll(rnn_model, name, [alphabet[\"S\"]]))\n",
    "print(\"Average NLL: \", total / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zly\n",
      "\n",
      "\n",
      "rwhanbwvflc\n",
      "xszlacdycbclknknknewanol\n",
      "inai\n",
      "h\n",
      "gezzyhlh\n",
      "\n",
      "w\n",
      "lyzayoulyw\n",
      "ueztrrzuzlrlhz\n",
      "nirk\n",
      "trin\n",
      "\n",
      "lrhh\n",
      "d\n",
      "iurrhyeatizevlnwlnlee\n",
      "hd\n",
      "iuzzbhsirarwzalulosuye\n",
      "humzruazh\n",
      "hl\n",
      "ahrsezehlzki\n",
      "ncylecurirkvnlileezd\n",
      "\n",
      "\n",
      "eh\n",
      "azh\n",
      "fyw\n",
      "ovihhe\n",
      "wbzge\n",
      "nizlaneel\n",
      "lunlazaolitbzlzbz\n",
      "lhelc\n",
      "\n",
      "ioynvebzibeb\n",
      "iza\n",
      "lu\n",
      "ar\n",
      "nmhhr\n",
      "ziha\n",
      "zenbrhnmzeebgenazoculvus\n",
      "lrznaleuefb\n",
      "irw\n",
      "cdzvxifurneer\n",
      "ly\n",
      "eoeu\n",
      "\n",
      "\n",
      "p\n",
      "l\n",
      "r\n",
      "rlerl\n",
      "oliwzcuah\n",
      "salihhicucl\n",
      "hnl\n",
      "oiu\n",
      "\n",
      "hb\n",
      "rlasiiurzbdny\n",
      "ziziraisnz\n",
      "eveeh\n",
      "neeiihdaa\n",
      "hcoekakbc\n",
      "lorzk\n",
      "llaurls\n",
      "l\n",
      "\n",
      "i\n",
      "stmhidtdlkrhci\n",
      "\n",
      "l\n",
      "ra\n",
      "liedehhdrzkh\n",
      "t\n",
      "rlrrhsln\n",
      "dr\n",
      "brsehwez\n",
      "yseyodee\n",
      "\n",
      "yythid\n",
      "hrzlm\n",
      "ebhezdn\n",
      "riimi\n",
      "c\n",
      "eerht\n",
      "ysuosiyezlxleygazlzislle\n",
      "ikxhzrezoclnilzuljez\n",
      "yiosye\n",
      "zl\n",
      "thhoh\n",
      "reedohssozkza\n",
      "rcl\n",
      "cizeidzv\n",
      "ihefzwqerdk\n",
      "hz\n",
      "siehuohgz\n",
      "hhitzabzhze\n",
      "dani\n",
      "\n",
      "Average NLL:  20.02976134300232\n"
     ]
    }
   ],
   "source": [
    "# As expected, the untrained RNN outputs trash. Let's train.\n",
    "for layer in rnn_model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "opt = torch.optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "for name in bigram_names:\n",
    "    rnn_model.zero_grad()\n",
    "    rnn_model.detach_h()\n",
    "    loss = 0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        probs = rnn_model.forward(bigram[0])\n",
    "        loss += -torch.log(probs[bigram[1]])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "test_model(rnn_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Builtin RNN for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26,  0,  0, 12,  8, 17, 27])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "Epoch:  0  Loss:  3.345205545425415\n",
      "Epoch:  1  Loss:  3.149601459503174\n",
      "Epoch:  2  Loss:  2.963387966156006\n",
      "Epoch:  3  Loss:  2.8448896408081055\n",
      "Epoch:  4  Loss:  2.7497055530548096\n"
     ]
    }
   ],
   "source": [
    "# Looks like builtin RNN does the same, maybe just not useful for bigram for some reason.\n",
    "# Try utilizing full sequence, first with builtin RNN\n",
    "# Restarting here, not going to use much of the above code. Want to try again from scratch.\n",
    "class RNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(len(alphabet), len(alphabet))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X is matrix of size (seq_len, len(alphabet))\n",
    "        output, _ = self.rnn(X, torch.randn(1, len(alphabet)))\n",
    "\n",
    "        return self.softmax(output)\n",
    "\n",
    "def name_to_matrix(name: list[int]) -> list[list[int]]:\n",
    "    return torch.stack([one_hot_encode_letter(token) for token in name])\n",
    "\n",
    "name_matrices = [name_to_matrix(name) for name in bigram_names]\n",
    "rnn_model = RNNv2()\n",
    "\n",
    "# Maybe problem from above was you weren't batching. At minimum should do multiple epochs through entire dataset with just one loss total\n",
    "opt = torch.optim.Adam(rnn_model.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    rnn_model.zero_grad()\n",
    "    for name_matrix in name_matrices:\n",
    "        # Predict a random letter in the name\n",
    "        output = rnn_model(name_matrix)\n",
    "        \n",
    "        for i in range(output.shape[0]):\n",
    "            loss += -torch.log(0.00001 + output[i][torch.nonzero(name_matrix[i])])\n",
    "            count += 1\n",
    "    \n",
    "    loss /= count    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch: \", epoch, \" Loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jrdrlhvoe\n",
      "hlhsvzebmmottyih\n",
      "\n",
      "pypeetnlln\n",
      "iroiyomhhpsgkolyvdjzq\n",
      "yqtgxilej\n",
      "rlqhiehmdeyinoeatrls\n",
      "bl\n",
      "iifrnctdys\n",
      "irepnnbxyenriebkkoinbokk\n",
      "ve\n",
      "bea\n",
      "zvynfei\n",
      "cgtjc\n",
      "kbeopp\n",
      "vavsrsiuttcthkgn\n",
      "zxlog\n",
      "izlinsaxnhd\n",
      "z\n",
      "lrumbaanmnmu\n",
      "p\n",
      "seaiilzl\n",
      "llikcefvlazk\n",
      "xhbeelldchfuzhiexxdtljwt\n",
      "ojwanaccbcprmmhso\n",
      "wmbpieswieaanxohzoommmk\n",
      "gofm\n",
      "ifvpuengoqkhrai\n",
      "j\n",
      "leentbemwsllmllll\n",
      "ignumioccormjaiinlmrocoa\n",
      "kepead\n",
      "yuvyuplillrl\n",
      "\n",
      "sfoemenbvreynlnuelhpke\n",
      "oaz\n",
      "mgxainmaarra\n",
      "cayencsaqeecemeqiw\n",
      "xtnacilrcrrcokbbo\n",
      "llacbbqbcckso\n",
      "o\n",
      "rma\n",
      "ejevzs\n",
      "\n",
      "ctacqr\n",
      "qqksweee\n",
      "ml\n",
      "daocvtopdno\n",
      "myjeuupnssclu\n",
      "houpbaoowln\n",
      "oaaebilykzbklrurhndsa\n",
      "avwdmvlospnsdinnlhly\n",
      "ntblmmfadyebnyarl\n",
      "ynarll\n",
      "iowu\n",
      "yaag\n",
      "ldgskfievzdle\n",
      "lbriispmmqmnnvxzmd\n",
      "juadedonvblzyl\n",
      "adilrxe\n",
      "koeereunphipwcstwlatcs\n",
      "diwgknjvcor\n",
      "imbkpiavlpueoletnrrbrzw\n",
      "kahvnenobeamlflnscbdqmke\n",
      "qdbicee\n",
      "prqslma\n",
      "gjzcemmajp\n",
      "\n",
      "oqmrm\n",
      "fxzrlkrwynkshe\n",
      "uboulq\n",
      "l\n",
      "bwknbye\n",
      "lao\n",
      "lrcugpt\n",
      "nlslltipgsee\n",
      "dssqcvunlacyummnrgs\n",
      "xxh\n",
      "anri\n",
      "\n",
      "mxaekbrbpbb\n",
      "jrfzsrarxp\n",
      "oreyepbixe\n",
      "eciwd\n",
      "roh\n",
      "\n",
      "jhhtohw\n",
      "ji\n",
      "ievazeolmmrlkkerroabyye\n",
      "r\n",
      "yni\n",
      "r\n",
      "bwiius\n",
      "mbnlkkinmsanxurwa\n",
      "\n",
      "wyuixsindalonnlivn\n",
      "hntbouh\n",
      "z\n",
      "exjhlnnc\n",
      "jjfoqniayqnfose\n",
      "Average NLL:  34.84290882349014\n"
     ]
    }
   ],
   "source": [
    "test_model(rnn_model, parse_output=lambda x: x[-1], prep_input=lambda x: name_to_matrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now begins ATTENTION MECHANISM\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, seq_len: int, v_size: int, kq_size: int):\n",
    "        super().__init__()\n",
    "        # Note that you can shrink the dimension of the query, key, and value vectors\n",
    "        self.W_k = nn.Parameter(torch.randn(kq_size, embed_size))\n",
    "        self.W_q = nn.Parameter(torch.randn(kq_size, embed_size))\n",
    "        self.W_v = nn.Parameter(torch.randn(v_size, embed_size))\n",
    "        self.d_k = torch.tensor(embed_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X represents a sequence. Each column is the embedding for a token.\n",
    "        # X is (m x s), where m is the embedding size, and s is the sequence length\n",
    "        # K, Q are the same size (m x s). V corresponds to output (n x s)\n",
    "\n",
    "        K = self.W_k @ X # kq_size x s\n",
    "        Q = self.W_q @ X # kq_size x s\n",
    "        V = self.W_v @ X # v_size x s\n",
    "        # You're not adding bias here, but that might be fine, because you want same mean of distribution?\n",
    "\n",
    "        dot = K.T @ Q # (s x kq)(kq x s) s x s to represent attention between each pair\n",
    "        dot /= torch.sqrt(self.d_k) # Because dot product of vectors with components distributed as N(0, 1) is distributed as N(0, d), and we want N(0, 1)\n",
    "\n",
    "        # We softmax over the rows, to get an s x 1 column vector. Then each column of V is multiplied by the corresponding entry.\n",
    "        # Is this broadcasted right?\n",
    "        # To get really good at this, want to learn a) broadcasting rules and b) figure out symmetries of matrix multiplication\n",
    "        z = V * torch.softmax(dot, dim=1)\n",
    "\n",
    "        return z # v_size x s\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, seq_len: int, num_heads: int, v_size: int, kq_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(embed_size, seq_len, v_size, kq_size) for _ in range(num_heads)])\n",
    "\n",
    "        # Head outputs (v_size x s) are concatenated into a (v_size * num_heads x s) matrix. And want (v_size x s) output, so multiply by (v_size x v_size * num_heads)\n",
    "        self.W = nn.Parameter(torch.randn(v_size, v_size * num_heads))\n",
    "        self.d_k = torch.tensor(embed_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = torch.cat([head(X) for head in self.heads], dim=0) # (v_size * num_heads x s)\n",
    "        output = self.W @ z\n",
    "\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Positional encoding\n",
    "\n",
    "        # Attention\n",
    "        # Calculate K, V, Q by separate linear layers from X\n",
    "\n",
    "        # Batch normalization\n",
    "\n",
    "        # Feedforward\n",
    "\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
