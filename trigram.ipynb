{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alphabet = {chr(i): i - 97 for i in range(97, 123)}\n",
    "alphabet[\"S\"] = 26; alphabet[\"E\"] = 27\n",
    "rev_alphabet = {v: k for k, v in alphabet.items()}\n",
    "\n",
    "def get_bigram_counts_with_delay(delay: int, names: list[torch.Tensor]):\n",
    "    counts = torch.ones(len(alphabet), len(alphabet), dtype=torch.float) # .ones for smoothing\n",
    "    for name in names:\n",
    "        for bigram in zip(name, name[delay:]):\n",
    "            counts[bigram[0], bigram[1]] += 1\n",
    "    counts = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return counts\n",
    "\n",
    "def build_dataset(max_delay: int):\n",
    "    names = [\"S\" * max_delay + name.strip().lower() + \"E\" for name in open(\"data/names.txt\").readlines()]\n",
    "    names = list(filter(lambda x: \"-\" not in x and \" \" not in x, names))\n",
    "\n",
    "    # Build dataset\n",
    "    names = [\"S\" + name.strip().lower() + \"E\" for name in open(\"data/names.txt\", \"r\").readlines()]\n",
    "    names = list(filter(lambda x: \"-\" not in x, names))\n",
    "\n",
    "    # Convert names to tensors\n",
    "    names = [torch.tensor([alphabet[char] for char in name]) for name in names]\n",
    "    \n",
    "    # Create the counts matrix for each delay\n",
    "    counts_per_delay = [get_bigram_counts_with_delay(delay, names) for delay in range(1, max_delay + 1)]\n",
    "\n",
    "    return names, counts_per_delay\n",
    "\n",
    "def display_name(name: list[int]):\n",
    "    return \"\".join([rev_alphabet[i] for i in name]).replace(\"S\", \"\").replace(\"E\", \"\")\n",
    "\n",
    "bigram_names, bigram_counts = build_dataset(1)\n",
    "delay_names, delay_counts = build_dataset(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Models: Bigram, Delay, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A series of models that give a probability distribution over the next character\n",
    "from functools import reduce\n",
    "from torch import nn\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "\n",
    "    def forward(self, name: list[int]):\n",
    "        probs = self.counts[0][name[-1], :]\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n",
    "\n",
    "class DelayModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "    \n",
    "    def forward(self, name: list[int]):\n",
    "        rows = [\n",
    "            self.counts[i][name[-(i + 1)], :]\n",
    "            for i in range(len(self.counts))\n",
    "        ]\n",
    "\n",
    "        probs = reduce(lambda a, b: a * b, rows)\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n",
    "\n",
    "class NNDelayModel(nn.Module):\n",
    "    def __init__(self, counts):\n",
    "        super().__init__()\n",
    "        self.counts = counts\n",
    "    \n",
    "    def forward(self, name: list[int]):\n",
    "        rows = [\n",
    "            self.counts[i][name[-(i + 1)], :]\n",
    "            for i in range(len(self.counts))\n",
    "        ]\n",
    "\n",
    "        probs = reduce(lambda a, b: a * b, rows)\n",
    "        return nn.functional.normalize(probs, p=1.0, dim=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a model, do some testing of it\n",
    "def generate_name(model: torch.nn.Module, initial: list[int], parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    name = [*initial] # Make a copy, otherwise you're updating the actual object, which is passed by pointer\n",
    "    i = 0\n",
    "    probs = ''\n",
    "    while name[-1] != alphabet[\"E\"] and i < 25:\n",
    "        probs = parse_output(model(prep_input(name)))\n",
    "        next_letter_idx = torch.multinomial(probs, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "        i += 1\n",
    "\n",
    "    return name[1:-1]\n",
    "\n",
    "def name_nll(model: torch.nn.Module, name: list[int], s_buffer: list[int], parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    if isinstance(name, torch.Tensor):\n",
    "        name = name.tolist()\n",
    "    nll = 0\n",
    "    full_name = s_buffer + name\n",
    "    s_buffer_len = len(full_name) - len(name)\n",
    "    \n",
    "    for i in range(len(name)):\n",
    "        probs = parse_output(model(prep_input(full_name[:s_buffer_len + i])))\n",
    "\n",
    "        if not torch.sum(probs).isclose(torch.ones(())):\n",
    "            raise Exception(\"ERROR probs don't add to one: \" + str(torch.sum(probs).item()), probs)\n",
    "\n",
    "        nll += -torch.log(probs[name[i]])\n",
    "\n",
    "    return nll.item()\n",
    "\n",
    "def test_model(model, prefix=[alphabet[\"S\"]], iters=100, parse_output=lambda x: x, prep_input=lambda x: x):\n",
    "    for _ in range(iters):\n",
    "        name = generate_name(model, prefix, parse_output=parse_output, prep_input=prep_input)\n",
    "        print(display_name(name))\n",
    "    \n",
    "    total = 0\n",
    "    count = 0\n",
    "    for name in bigram_names:\n",
    "        nll = name_nll(model, name, prefix, parse_output=parse_output, prep_input=prep_input)\n",
    "        total += nll\n",
    "        count += 1\n",
    "\n",
    "    print(\"Average NLL: \", total / count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had\n",
      "mot\n",
      "tas\n",
      "\n",
      "odazinastio\n",
      "c\n",
      "mmissarthuilq\n",
      "madar\n",
      "ss\n",
      "ht\n",
      "wrushereorinar\n",
      "juarelbe\n",
      "lustulvpirsamacle\n",
      "momverinertharizawarof\n",
      "laloran\n",
      "dy\n",
      "an\n",
      "peryars\n",
      "inatiernnolle\n",
      "m\n",
      "budreranelle\n",
      "feeer\n",
      "beriuamalbre\n",
      "jus\n",
      "madrlerooshrtt\n",
      "heymamaimee\n",
      "schaniusit\n",
      "foshabwi\n",
      "binnondg\n",
      "heshavad\n",
      "coudier\n",
      "mmsarastoudort\n",
      "gond\n",
      "rqurstzery\n",
      "llmagierdricle\n",
      "hewililarl\n",
      "wicylin\n",
      "b\n",
      "guere\n",
      "rie\n",
      "r\n",
      "etes\n",
      "s\n",
      "vs\n",
      "rosh\n",
      "sal\n",
      "wclfrarienntcqx\n",
      "rckinanistorin\n",
      "itrimitan\n",
      "gtcy\n",
      "budo\n",
      "txico\n",
      "gy\n",
      "joscan\n",
      "iabdrvierd\n",
      "tetthiavorlvexm\n",
      "cap\n",
      "shaud\n",
      "garxby\n",
      "ty\n",
      "rariesban\n",
      "stuwr\n",
      "wnen\n",
      "mobosordexerckericarins\n",
      "har\n",
      "giatebrbal\n",
      "st\n",
      "obrhan\n",
      "stb\n",
      "bbantot\n",
      "s\n",
      "oh\n",
      "ffefl\n",
      "hokit\n",
      "llien\n",
      "gndery\n",
      "ghrken\n",
      "s\n",
      "aulliexindathanestharilm\n",
      "ham\n",
      "josepae\n",
      "e\n",
      "oxewariede\n",
      "edonobbr\n",
      "sh\n",
      "mond\n",
      "c\n",
      "murba\n",
      "rl\n",
      "qulard\n",
      "loamoncl\n",
      "ln\n",
      "fel\n",
      "bydus\n",
      "pqan\n",
      "wishnd\n",
      "ory\n",
      "ahthanen\n",
      "michen\n",
      "balbrd\n",
      "Average NLL:  25.06748729982841\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramModel(bigram_counts)\n",
    "test_model(bigram_model, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rie\n",
      "rrellie\n",
      "iera\n",
      "murie\n",
      "eie\n",
      "aser\n",
      "rem\n",
      "ar\n",
      "rando\n",
      "arr\n",
      "err\n",
      "alen\n",
      "rda\n",
      "ele\n",
      "ete\n",
      "erig\n",
      "ari\n",
      "lar\n",
      "ari\n",
      "arie\n",
      "ell\n",
      "rar\n",
      "erie\n",
      "ann\n",
      "ier\n",
      "aad\n",
      "are\n",
      "are\n",
      "erie\n",
      "ale\n",
      "har\n",
      "are\n",
      "ron\n",
      "arr\n",
      "are\n",
      "rrn\n",
      "ale\n",
      "har\n",
      "rre\n",
      "rin\n",
      "er\n",
      "arnn\n",
      "ran\n",
      "rial\n",
      "ell\n",
      "ran\n",
      "arrin\n",
      "ral\n",
      "arae\n",
      "dar\n",
      "are\n",
      "ire\n",
      "are\n",
      "lar\n",
      "eor\n",
      "rir\n",
      "maer\n",
      "las\n",
      "err\n",
      "aue\n",
      "arn\n",
      "rac\n",
      "eren\n",
      "allan\n",
      "rea\n",
      "aerie\n",
      "eer\n",
      "are\n",
      "ale\n",
      "ayle\n",
      "ardie\n",
      "lare\n",
      "ole\n",
      "erie\n",
      "ara\n",
      "rry\n",
      "ris\n",
      "rro\n",
      "rin\n",
      "rel\n",
      "ran\n",
      "lar\n",
      "and\n",
      "ell\n",
      "arden\n",
      "are\n",
      "arr\n",
      "ter\n",
      "ene\n",
      "ran\n",
      "aro\n",
      "arro\n",
      "sanee\n",
      "adle\n",
      "onn\n",
      "ron\n",
      "arrie\n",
      "ree\n",
      "are\n",
      "rin\n",
      "Average NLL:  40.08920456905476\n"
     ]
    }
   ],
   "source": [
    "delay_model = DelayModel(delay_counts)\n",
    "test_model(delay_model, prefix=3*[alphabet[\"S\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network - scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ixdgxsetsyssxqxqskeizzei\n",
      "pgclipxgylqxdszsieminsxq\n",
      "xsyxmggxjxggszvmgomxggdx\n",
      "xgyxibdxzygimixixggmmngg\n",
      "iamxpixlgpdxxgxmgpsxpkxi\n",
      "gcdsksgasgomgcmxaaglxilx\n",
      "dlxaxpgemxxsxasxsyxqskex\n",
      "cmgpsxaslxyxgesgoiqgzqkx\n",
      "ggbdsgmssxgmsclxxxxxlxxk\n",
      "ixgylxzzggysxixnbqxemged\n",
      "xpmxpwgdsxwxgsgssxqsxsss\n",
      "egomscsgssqscsqxesqqsqds\n",
      "aqegcjsixsgyqzzzzgzziqqz\n",
      "ggl\n",
      "aixyxpxeicyiqipqiqrgzsio\n",
      "xmqxdgxgsagnxixkgxclxxgx\n",
      "gxsxaszqeksezggmzeagegim\n",
      "cmxaixdgxixpglslywxgplix\n",
      "yggydbebzeemgimsxnxgimxb\n",
      "ixxgyxdsygysxsqqsszqegim\n",
      "wsqxyxygezzzezeaezeizeiz\n",
      "ec\n",
      "phxsixygyqxqqzqqgqqqqqqs\n",
      "msjxsxyxygygygxgxxgnnxxk\n",
      "gmqscssgysekvgigcxigcgid\n",
      "dc\n",
      "saxmgxmxiilygxgpdmkxgxgx\n",
      "cqspsqqqyslqqqqqqgolmxkx\n",
      "geyyqczscceezzigggqqsngg\n",
      "ggpsxiqxgygqzqgqysgmqsks\n",
      "zogmsxsgxsxsggmxqxgskggx\n",
      "mgbxslxqydewezzziegiescg\n",
      "psclixyyliyxxuibxiwzzzzf\n",
      "itipdsxisxiqxilszleemgem\n",
      "pxcxyxwglyzeqzqpzqzzzqqz\n",
      "yiqxixqgcnmxgmxcxgpdzpip\n",
      "cnosbuckxxgkgxxkggggggcp\n",
      "glspssxxgyxcglgilxygqqss\n",
      "qgcskegggnsclgxgxkixygpd\n",
      "nixxxmpkxxgxgggkxdgxyxgg\n",
      "ditixbspipikxggmiskgqssg\n",
      "mgpyxwgeyzegggiegggdxgps\n",
      "bsepszsxyzwgzzzrzqzqkezz\n",
      "ggljxdxcxwxaxexgxegysqzs\n",
      "pseygeqqkzegezeekezkzqiq\n",
      "gqsqsqzeeieggqxsxqgqgmxs\n",
      "oiyxjxbnbxnxxxxgnmxxngxg\n",
      "cciipsxysxqqsqqqqwgysqdq\n",
      "csqxigyizggmqxggggdxgmkx\n",
      "cybrxnmgnmnpmnwgzskxnxkk\n",
      "xxxmxawxgmxxsxyysgyszkge\n",
      "cmsksgygxglldpgxgpxlglhg\n",
      "msjxyxpgxgxgygygygygysog\n",
      "q\n",
      "jspqszmscwxxgqxdszgcsnxk\n",
      "ggasecsxymszqqkezezeieqi\n",
      "gcsxcylilkqedzxogegiexcx\n",
      "glixsgeszsykqqescegqqqzk\n",
      "goqxdzsgegixg\n",
      "yxggxgds\n",
      "tsxislmkxgkxgaeggixgipsz\n",
      "ggmxcwsxilxvyxgydpxgxixg\n",
      "pbvxscmindlpiltlxxxtkktk\n",
      "bxixmxpbxsyxxpxxibxipmx\n",
      "xggdscslxgxsgxwslsqssysc\n",
      "gcamxyxegzgcyziieckgggge\n",
      "zxgpksxoemgggskxgxsgxggq\n",
      "gsqxygqxcxkzgegiikgzzzzk\n",
      "nmsxnxxggygxdxxggiixsxkg\n",
      "scqqxysegzzqiqdqseiegiis\n",
      "pqxwgcszqngeqqzszqqzqiqs\n",
      "xiegyzqziqcdwgpqseikgqiz\n",
      "cmilixxglywxgyxgymxzdxpz\n",
      "mspgyxhszqxiyqgmqsgdgqgq\n",
      "cdscspsgysxyqsqxlxllxaxk\n",
      "igxsqxygqscdqcdsclxonxwx\n",
      "jxmxrxnxilkxxxxxxlxpsxpk\n",
      "hxnbxygzikxggkxegmxgmxxw\n",
      "mxjnxdggkxpgxskxkxgxxxkx\n",
      "gysqqsgdgzyseeseeekigglg\n",
      "gsxgqxcszskgedexieagpgge\n",
      "cgxasliqxqwlgmsxlyqxgyqx\n",
      "dxowmgbxbxygmxixxgmxbxxp\n",
      "jxkxyxggzscekemxegpswggz\n",
      "esqjssksgxzszkemzeigggxq\n",
      "clxsvkskyxygeskeegzzipzg\n",
      "sgxqqqsclgolvwxyscxnxgxw\n",
      "cmxhixzpixdsxldxglxxbxxy\n",
      "pixsgqsgysclqxdxwmqxgxqx\n",
      "gysqsqzgekiixgmgziqxdggi\n",
      "cggxsipsgyqxwdxygzsgygeq\n",
      "egyxzpbsqzzzszeszsczqesq\n",
      "injggyxxdxlxxxxxdxxxxxxx\n",
      "mxbxnmxxgpbxxaxxgxgzxggy\n",
      "mseycliripyzidxsixxrxkli\n",
      "pbutzpszzkiixagggmmxkggk\n",
      "nysxbxxixxiikgkkixkxpgxg\n",
      "\n",
      "xvxpixgclixkdxdxiipilxip\n",
      "ixsppygyqxuzcgzzgizgiesz\n",
      "Average NLL:  94.75779801182401\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "def one_hot_encode_letter(letter: int):\n",
    "    one_hot = torch.zeros(len(alphabet))\n",
    "    one_hot[letter] = 1\n",
    "    return one_hot\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, param_size: int):\n",
    "        super().__init__()\n",
    "        self.U = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.W = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.V = nn.Parameter(torch.randn(param_size, param_size))\n",
    "        self.h = torch.randn(param_size)\n",
    "        self.bias_h = nn.Parameter(torch.randn(param_size))\n",
    "        self.bias_o = nn.Parameter(torch.randn(param_size))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    # x should be a one-hot vector\n",
    "    def forward(self, x):\n",
    "        x = one_hot_encode_letter(x)\n",
    "        self.h = torch.tanh(self.U @ x + self.W @ self.h + self.bias_h)\n",
    "        return self.softmax(self.bias_o + self.V @ self.h)\n",
    "\n",
    "    def detach_h(self):\n",
    "        self.h = self.h.detach()\n",
    "\n",
    "rnn_model = RNN(len(alphabet))\n",
    "test_model(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "eahne\n",
      "whxe\n",
      "ephrruzksoekggggogegegii\n",
      "onll\n",
      "rxp\n",
      "uztgokoggcigcilkikgegegg\n",
      "gclgcllxdxgirxxysjpseqew\n",
      "qgyiyxgddzaildlnilldwlgd\n",
      "lgyxaykoalailgrlglgelied\n",
      "lienri\n",
      "hoial\n",
      "aukehyhogsezekelgelkegeg\n",
      "oalxiltxpilgldlgllllmlxi\n",
      "ccdlolggxaileoxgxxxareh\n",
      "hkokpiea\n",
      "aourze\n",
      "ua\n",
      "hoaaoulecjresapweiegeseq\n",
      "oqxdaehdhkrlleoea\n",
      "oaoa\n",
      "hwtvkeazeeeiyicgkzoiqiee\n",
      "onlnylphxkxlielwxggedeeg\n",
      "gilxnldxlgnxlxlileilkgee\n",
      "gkxklaeyxxieaoegelgldeeg\n",
      "g\n",
      "ilxhpdxpxilxwegxszqqxegq\n",
      "cgcysxysxomxndxgdklgekil\n",
      "lkxilleixrenen\n",
      "it\n",
      "e\n",
      "aaevreeoegiesieee\n",
      "aihztasekzqgqeeeeaoeiale\n",
      "gglxhlqxdskoklloalealaxi\n",
      "ggcxilgonlldxlllipxwetkx\n",
      "lxdxx\n",
      "aewjroh\n",
      "e\n",
      "lwhapezzqqqzpsqzzeiqsege\n",
      "gpdcasxyslollipxgxdxwlke\n",
      "cnno\n",
      "royayaw\n",
      "adr\n",
      "iaaeran\n",
      "ochawajoehjzsaweahfdxtxd\n",
      "x\n",
      "m\n",
      "ayawawu\n",
      "hh\n",
      "ru\n",
      "o\n",
      "aza\n",
      "\n",
      "eeheoiatwuhcrurszjzskiks\n",
      "oglsexyxeitxpgyszoyxqqzq\n",
      "oidcxllttpilxxlitlxflpix\n",
      "jyhfre\n",
      "aexo\n",
      "\n",
      "nzhzalaaydetuzxzekoexegg\n",
      "gtxliolgyxpgyyxildxldxxg\n",
      "xe\n",
      "eewfrhjhezufsiftsqzsqbse\n",
      "xggcnlurxhelkekzkokgkezl\n",
      "lorextllkeiieioisipeklwe\n",
      "rmiaarbe\n",
      "wyyeuhrhyipzzzkgzkmesgeg\n",
      "oilxixgxmxglemhrhxethpix\n",
      "ggysy\n",
      "egodstisxogqttxgyqxdizon\n",
      "mcllgxcxpjzzqxqiyxkcedzz\n",
      "olxusgxslkekkgsigggeggee\n",
      "cneuhkxggceglgxelgggeggg\n",
      "gyxkieggyoedggtlxygegexi\n",
      "gglddesoogggxtixxgqyskey\n",
      "ggylxngxgelilxggysgpkxws\n",
      "gixlxwwegdxwseqqsqizxqsz\n",
      "oyglhxhzyszkgxedxyxkilel\n",
      "\n",
      "nelikxegelgxgegebekegeeg\n",
      "geszomgaseiggglxieqkldeg\n",
      "linpzlneklixklilkzcgkwig\n",
      "gcwxpszgqqqqiqmqiqxdqcgq\n",
      "cgxogxnyougmntzkzzkgggkg\n",
      "gxlmaolokkllalewewgegeig\n",
      "gliedgxmxtiqspszoloixsxn\n",
      "cognlxwlxgxlxwwlgklwwggl\n",
      "zuuhraoh\n",
      "rwl\n",
      "\n",
      "i\n",
      "aw\n",
      "ylaio\n",
      "wwaesipzxjqgxqzzsqeiqsge\n",
      "rirxrytkzopgylkyklydkalg\n",
      "gaslaelkeg\n",
      "tiliwlxwzilyieikokogdkzi\n",
      "gglslkkgszekloekkogxaelg\n",
      "ggxmxegkyougyeagokgggclz\n",
      "gnbaheeueeilkeeekeeeaeeg\n",
      "Average NLL:  38.27007855338283\n"
     ]
    }
   ],
   "source": [
    "# As expected, the untrained RNN outputs trash. Let's train.\n",
    "for layer in rnn_model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "opt = torch.optim.SGD(rnn_model.parameters(), lr=0.01)\n",
    "for name in bigram_names:\n",
    "    rnn_model.zero_grad()\n",
    "    rnn_model.detach_h()\n",
    "    loss = 0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        probs = rnn_model.forward(bigram[0])\n",
    "        loss += -torch.log(probs[bigram[1]])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "test_model(rnn_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Builtin RNN for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  3.332918405532837\n",
      "Epoch:  1  Loss:  3.0117368698120117\n",
      "Epoch:  2  Loss:  2.8645331859588623\n",
      "Epoch:  3  Loss:  2.842525005340576\n",
      "Epoch:  4  Loss:  2.7010793685913086\n",
      "Epoch:  5  Loss:  2.612403631210327\n",
      "Epoch:  6  Loss:  2.50447940826416\n",
      "Epoch:  7  Loss:  2.4084012508392334\n",
      "Epoch:  8  Loss:  2.3340466022491455\n",
      "Epoch:  9  Loss:  2.2692861557006836\n"
     ]
    }
   ],
   "source": [
    "# Looks like builtin RNN does the same, maybe just not useful for bigram for some reason.\n",
    "# Try utilizing full sequence, first with builtin RNN\n",
    "# Restarting here, not going to use much of the above code. Want to try again from scratch.\n",
    "class RNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(len(alphabet), len(alphabet))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X is matrix of size (seq_len, len(alphabet))\n",
    "        output, _ = self.rnn(X, torch.randn(1, len(alphabet)))\n",
    "\n",
    "        return self.softmax(output)\n",
    "\n",
    "def name_to_matrix(name: list[int]) -> list[list[int]]:\n",
    "    return torch.stack([one_hot_encode_letter(token) for token in name])\n",
    "\n",
    "name_matrices = [name_to_matrix(name) for name in bigram_names]\n",
    "rnn_model = RNNv2()\n",
    "\n",
    "# Maybe problem from above was you weren't batching. At minimum should do multiple epochs through entire dataset with just one loss total\n",
    "opt = torch.optim.Adam(rnn_model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    rnn_model.zero_grad()\n",
    "    for name_matrix in name_matrices:\n",
    "        # Predict a random letter in the name\n",
    "        output = rnn_model(name_matrix)\n",
    "        \n",
    "        for i in range(output.shape[0]):\n",
    "            loss += -torch.log(output[i][torch.nonzero(name_matrix[i])])\n",
    "            count += 1\n",
    "    \n",
    "    loss /= count    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Epoch: \", epoch, \" Loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "vp\n",
      "eccveetg\n",
      "dvkf\n",
      "o\n",
      "sg\n",
      "rgrrooxmvlyywdozivnh\n",
      "aa\n",
      "\n",
      "p\n",
      "viiipe\n",
      "isnzpynw\n",
      "w\n",
      "npa\n",
      "f\n",
      "ksora\n",
      "d\n",
      "agj\n",
      "\n",
      "bennnllfd\n",
      "i\n",
      "lvlhbylt\n",
      "re\n",
      "oiicariy\n",
      "z\n",
      "abengyyy\n",
      "lbgvgh\n",
      "qkmyaxj\n",
      "ssjtt\n",
      "hxsnwhq\n",
      "ieayyxcqyc\n",
      "olqewihtt\n",
      "\n",
      "fyffkxihp\n",
      "urgy\n",
      "\n",
      "ccll\n",
      "ukzttlmeuktbokuga\n",
      "jh\n",
      "\n",
      "mghvxld\n",
      "rv\n",
      "e\n",
      "rurwpbgennmtxllol\n",
      "louhds\n",
      "rp\n",
      "uoflkaswx\n",
      "bjlym\n",
      "zpnhmme\n",
      "wiltcrdsttomvexltvbdd\n",
      "\n",
      "g\n",
      "wocwow\n",
      "kewb\n",
      "zzzcypumyrdctpqi\n",
      "y\n",
      "pa\n",
      "shozpdetlj\n",
      "llflkemdcbaagqr\n",
      "ggql\n",
      "lp\n",
      "pmeaqbyeqbkrmaxiodxs\n",
      "anyw\n",
      "sdg\n",
      "lawrhyyhyhnhk\n",
      "fa\n",
      "uvhawzqo\n",
      "pemvhirpe\n",
      "n\n",
      "fgyfvsnipj\n",
      "s\n",
      "ta\n",
      "pem\n",
      "cj\n",
      "cnxllu\n",
      "\n",
      "rvbyvxdrwpnttgs\n",
      "kku\n",
      "j\n",
      "rrtidkgblw\n",
      "pcyaltyycmp\n",
      "hc\n",
      "t\n",
      "z\n",
      "sgergz\n",
      "\n",
      "toogsqnhelioruefbrydzk\n",
      "gjpnxs\n",
      "na\n",
      "jnuiiprzxiellla\n",
      "eliessi\n",
      "iqereyhhh\n",
      "\n",
      "etdqpl\n",
      "mcjceldd\n",
      "bmawdke\n",
      "f\n",
      "pe\n",
      "jthag\n",
      "i\n",
      "Average NLL:  23.968230487460616\n"
     ]
    }
   ],
   "source": [
    "test_model(rnn_model, parse_output=lambda x: x[-1], prep_input=lambda x: name_to_matrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ydrpyhx\n",
      "hwezzdlrkwcchbqmvdcqieq\n",
      "dp\n",
      "divkkvzilzglrhoopopeekj\n",
      "wienxfjczxpfc\n",
      "vvad\n",
      "qa\n",
      "sgvwxwazxdrewuacilkxiqo\n",
      "vppwadtymulxnfcknydnwfhg\n",
      "nfyxiesilkcubbqzhhiboxfq\n",
      "xxuu\n",
      "y\n",
      "pngmfcumvashatxfkgxjcfcl\n",
      "x\n",
      "xcwhgaxa\n",
      "xi\n",
      "xilxdkbrgzqitpvcoviinvh\n",
      "wacslxjriescmg\n",
      "wabxreequcrtpmpzbyzdzuxj\n",
      "dwikrpykxiwzjeroizxozpsd\n",
      "ywdbquogdubkbfcshgxgraf\n",
      "xciwhrzflwui\n",
      "xclhbhyuxh\n",
      "xaoizqcguoiltthjdrltfulg\n",
      "uahbxgxo\n",
      "zvzmweelfliyuaymweuiszxm\n",
      "wyvhsliilxhqwvwdmcaofn\n",
      "gpnyxzlicimmblwnzbhqopht\n",
      "bvfwpo\n",
      "\n",
      "fabgywdxxfa\n",
      "cdhfgrugfzdtkezhjeqlxagd\n",
      "yxrqlwuwciluonvyeasq\n",
      "wxbfnzedxxrkwovc\n",
      "eququarv\n",
      "ilzncxjsyhuj\n",
      "hyngvphvjlmpvbyuznili\n",
      "zikxwvfou\n",
      "xizgdfejxlqffdxineanoqu\n",
      "xalxpychqalcaxvadujreu\n",
      "kjcdxafrfnuwffai\n",
      "xjgdrqrakij\n",
      "xmakxxfizlyloasgvsdoqqn\n",
      "qwyyagpjmsxbhntzbefqghnt\n",
      "xadm\n",
      "auxpdqimoirmvgkpwnop\n",
      "lh\n",
      "xxv\n",
      "fgaxfgzv\n",
      "xcuzzxgieamjmiqzkohtpdp\n",
      "yywmpziwwbyp\n",
      "wewughbreddflomlyetvdkkn\n",
      "\n",
      "xljdwmelmouohlsfyktmingc\n",
      "wugtauuyx\n",
      "xxligibrgkjscgj\n",
      "ximfzixwohlrs\n",
      "zwalbmkic\n",
      "xdvriufwfr\n",
      "dgwvpfzbzwpmnapykorfgnnh\n",
      "vx\n",
      "bxlxoqckypxrervlyqzavm\n",
      "xwd\n",
      "yykxgkxxnznvpxom\n",
      "yxfggabyahiubcqmuxmfu\n",
      "wwvukouijtjuffchqbyea\n",
      "l\n",
      "xzapvfbravluhfrapxnavgd\n",
      "k\n",
      "auqiyfkbuagvtsrlxahhzzxe\n",
      "xzemtqeeqeddvqgfuavixmru\n",
      "cxgxzw\n",
      "uzyhryhkizzruopki\n",
      "vxufaaci\n",
      "kliqvvbaxpqrdkitpkt\n",
      "xhodnlziqiayvvsdtbgppcr\n",
      "zcjaqslmryxoknodlbtdz\n",
      "yvlkmtuypxnapfnuy\n",
      "xpxp\n",
      "sxqlh\n",
      "ccbqrig\n",
      "wecntquggfkaxcnfftuedbrb\n",
      "kxlm\n",
      "wxdftzvgund\n",
      "flxdw\n",
      "\n",
      "lgcoijmtpx\n",
      "xl\n",
      "elatwkxffyxgrycc\n",
      "xlqzbwhlccmkjmmksmsoyzou\n",
      "rwffzzveijvvseorbyvnz\n",
      "xwjruaxkyjfaaqcvbhaufiwr\n",
      "lllphrqt\n",
      "yqlxxi\n",
      "zvrpvachugfbrmukwru\n",
      "dyoi\n",
      "lvkemzkhksx\n",
      "hnnxyxdn\n",
      "yatghsckjxgkmxgbisfyqnib\n",
      "ydnegshar\n",
      "Average NLL:  29.797344200935456\n"
     ]
    }
   ],
   "source": [
    "# Now begins ATTENTION MECHANISM\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, seq_len: int, v_size: int, kq_size: int):\n",
    "        super().__init__()\n",
    "        # Note that you can shrink the dimension of the query, key, and value vectors\n",
    "        self.W_k = nn.Parameter(torch.randn(kq_size, embed_size))\n",
    "        self.W_q = nn.Parameter(torch.randn(kq_size, embed_size))\n",
    "        self.W_v = nn.Parameter(torch.randn(v_size, embed_size))\n",
    "        self.d_k = torch.tensor(embed_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X represents a sequence. Each column is the embedding for a token.\n",
    "        # X is (m x s), where m is the embedding size, and s is the sequence length\n",
    "        # K, Q are the same size (m x s). V corresponds to output (n x s)\n",
    "\n",
    "        K = self.W_k @ X # kq_size x s\n",
    "        Q = self.W_q @ X # kq_size x s\n",
    "        V = self.W_v @ X # v_size x s\n",
    "        # You're not adding bias here, but that might be fine, because you want same mean of distribution?\n",
    "\n",
    "        dot = K.T @ Q # (s x kq)(kq x s) s x s to represent attention between each pair\n",
    "        dot /= torch.sqrt(self.d_k) # Because dot product of vectors with components distributed as N(0, 1) is distributed as N(0, d), and we want N(0, 1)\n",
    "\n",
    "        # We softmax over the rows, to get an s x 1 column vector. Then each column of V is multiplied by the corresponding entry.\n",
    "        # Is this broadcasted right?\n",
    "        # To get really good at this, want to learn a) broadcasting rules and b) figure out symmetries of matrix multiplication\n",
    "        # print(\"K: \", K.shape)\n",
    "        # print(\"Q: \", Q.shape)\n",
    "        # print(\"V: \", V.shape)\n",
    "        # print(\"dot: \", dot.shape)\n",
    "\n",
    "        z = V @ torch.softmax(dot, dim=1) # I don't think this should be normal matrix multiplication\n",
    "\n",
    "        return self.softmax(z) # v_size x s\n",
    "\n",
    "attention_model = SingleHeadAttention(28, 10, 28, 16)\n",
    "test_model(attention_model, parse_output=lambda x: x[:, -1], prep_input=lambda x: name_to_matrix(x).T)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size: int, seq_len: int, num_heads: int, v_size: int, kq_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(embed_size, seq_len, v_size, kq_size) for _ in range(num_heads)])\n",
    "\n",
    "        # Head outputs (v_size x s) are concatenated into a (v_size * num_heads x s) matrix. And want (v_size x s) output, so multiply by (v_size x v_size * num_heads)\n",
    "        self.W = nn.Parameter(torch.randn(v_size, v_size * num_heads))\n",
    "        self.d_k = torch.tensor(embed_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = torch.cat([head(X) for head in self.heads], dim=0) # (v_size * num_heads x s)\n",
    "        output = self.W @ z\n",
    "\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Positional encoding\n",
    "\n",
    "        # Attention\n",
    "        # Calculate K, V, Q by separate linear layers from X\n",
    "\n",
    "        # Batch normalization\n",
    "\n",
    "        # Feedforward\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
