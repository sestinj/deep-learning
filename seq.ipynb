{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Build dataset\n",
    "names = [\"S\" + name.strip().lower() + \"E\" for name in open(\"data/names.txt\", \"r\").readlines()]\n",
    "names = list(filter(lambda x: \"-\" not in x, names))\n",
    "alphabet = {chr(i): i - 97 for i in range(97, 123)}\n",
    "alphabet[\" \"] = 26; alphabet[\"S\"] = 27; alphabet[\"E\"] = 28\n",
    "rev_alphabet = {v: k for k, v in alphabet.items()}\n",
    "\n",
    "# Convert names to tensors\n",
    "names = [torch.tensor([alphabet[char] for char in name]) for name in names]\n",
    "\n",
    "counts = torch.zeros(len(alphabet), len(alphabet), dtype=torch.float)\n",
    "for name in names:\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        counts[bigram[0], bigram[1]] += 1\n",
    "\n",
    "# Normalize the rows of counts\n",
    "counts = counts / counts.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_prob(name: list[int]) -> float:\n",
    "    prob = 1.0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        prob *= counts[bigram[0], bigram[1]].item()\n",
    "    return prob\n",
    "\n",
    "def name_log_prob(name: list[int]) -> float:\n",
    "    bigrams = zip(name, name[1:])\n",
    "    # Vectorize\n",
    "    return -torch.sum(torch.log(counts[name[:-1], name[1:]])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyaminamselerkeonrd\n",
      "52.25434112548828\n",
      "avondy\n",
      "13.929759979248047\n",
      "iles\n",
      "7.177988052368164\n",
      "selen\n",
      "8.928884506225586\n",
      "gantther\n",
      "14.41200065612793\n",
      "\n",
      "=====REAL NAMES=====\n",
      "SaamirE\n",
      "18.659114837646484\n",
      "SaaronE\n",
      "14.553874969482422\n",
      "SabbeyE\n",
      "14.094061851501465\n",
      "SabbieE\n",
      "15.160078048706055\n",
      "SabbotE\n",
      "17.108291625976562\n"
     ]
    }
   ],
   "source": [
    "def generate_name_bigram(counts: torch.TensorType):\n",
    "    name = [alphabet[\"S\"]]\n",
    "    while name[-1] != alphabet[\"E\"]:\n",
    "        row = counts[name[-1], :]\n",
    "        next_letter_idx = torch.multinomial(row, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "    return name[1:-1]\n",
    "\n",
    "def display_name(name: list[int]):\n",
    "    return \"\".join([rev_alphabet[idx] for idx in name])\n",
    "\n",
    "for i in range(5):\n",
    "    name = generate_name_bigram(counts)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))\n",
    "print(\"\\n=====REAL NAMES=====\")\n",
    "for i in range(5):\n",
    "    print(display_name(names[i].tolist()))\n",
    "    print(name_log_prob(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogjgycgnjmhwlnonlfS\n",
      "inf\n",
      "gwnaSkdlp odmrmnvbS\n",
      "nan\n",
      "bmSfhmkw\n",
      "inf\n",
      "n\n",
      "-0.0\n",
      "cfufymrm\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# Figure out the counts using NN\n",
    "from torch import nn\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, hidden_size) # Would use embedding if you had a larger vocab\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=0) # Why dim=1?\n",
    "\n",
    "    def forward(self, input: torch.TensorType) -> torch.TensorType:\n",
    "        output = self.linear(input)\n",
    "        return self.softmax(output)\n",
    "\n",
    "def generate_name_nn(model: BigramModel):\n",
    "    name = [alphabet[\"S\"]]\n",
    "    i = 0\n",
    "    while name[-1] != alphabet[\"E\"] and i < 20:\n",
    "        # One-hot encoding of last letter\n",
    "        input = torch.zeros(model.vocab_size)\n",
    "        input[name[-1]] += 1\n",
    "        # Outputs probabilities row\n",
    "        probs = model.forward(input)\n",
    "        # Sample next letter\n",
    "        next_letter_idx = torch.multinomial(probs, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "        i += 1\n",
    "\n",
    "    return name[1:-1]\n",
    "\n",
    "model = BigramModel(len(alphabet))\n",
    "for i in range(5):\n",
    "    name = generate_name_nn(model)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for name in names:\n",
    "    # Reset gradient\n",
    "    model.zero_grad()\n",
    "    # Compute loss\n",
    "    loss = 0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        input = torch.zeros(model.vocab_size)\n",
    "        input[bigram[0]] += 1\n",
    "        probs = model.forward(input)\n",
    "        loss += -torch.log(probs[bigram[1]]) # Negative log likelihood loss\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    # Update parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiyqtdeoiepaajleha\n",
      "inf\n",
      "baiterchndwt\n",
      "32.55701446533203\n",
      "tl\n",
      "4.352035999298096\n",
      "otarru\n",
      "14.201091766357422\n",
      "n\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    name = generate_name_nn(model)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should try doing a trigram model. There are probably many increasingly clever ways to do this, worth making a few up before googling the canonical solutions.\n",
    "# Keep a delayed bigram and use this along with the normal bigram (average the rows, then take multinomial sample)\n",
    "# Simply use a 3D tensor (fine for trigram, not for much more)\n",
    "# Make an embedding of the last n letters\n",
    "# Use a recurrent neural network\n",
    "# You should try to make your owv version of the attention mechanism from memory, or whatever you think the word \"attention\" SHOULD mean. Then see how this later compares to an actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with RNN - This is a prebuilt model that you can test against. But should build your own.\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, hidden_size) # Would use embedding if you had a larger vocab\n",
    "        self.rnn = nn.RNN(vocab_size, vocab_size)\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, input: torch.TensorType, hidden: torch.TensorType) -> torch.TensorType:\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.linear(output)\n",
    "        return self.softmax(output), hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
