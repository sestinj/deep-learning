{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Build dataset\n",
    "names = [\"S\" + name.strip().lower() + \"E\" for name in open(\"data/names.txt\", \"r\").readlines()]\n",
    "names = list(filter(lambda x: \"-\" not in x, names))\n",
    "alphabet = {chr(i): i - 97 for i in range(97, 123)}\n",
    "alphabet[\" \"] = 26; alphabet[\"S\"] = 27; alphabet[\"E\"] = 28\n",
    "rev_alphabet = {v: k for k, v in alphabet.items()}\n",
    "\n",
    "# Convert names to tensors\n",
    "names = [torch.tensor([alphabet[char] for char in name]) for name in names]\n",
    "\n",
    "counts = torch.zeros(len(alphabet), len(alphabet), dtype=torch.float)\n",
    "for name in names:\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        counts[bigram[0], bigram[1]] += 1\n",
    "\n",
    "# Normalize the rows of counts\n",
    "counts = counts / counts.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_prob(name: list[int]) -> float:\n",
    "    prob = 1.0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        prob *= counts[bigram[0], bigram[1]].item()\n",
    "    return prob\n",
    "\n",
    "def name_log_prob(name: list[int]) -> float:\n",
    "    bigrams = zip(name, name[1:])\n",
    "    # Vectorize\n",
    "    return -torch.sum(torch.log(0.01 + counts[name[:-1], name[1:]])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiled\n",
      "8.731794357299805\n",
      "ge\n",
      "1.620269775390625\n",
      "gy\n",
      "3.860867977142334\n",
      "pardofoun\n",
      "19.488985061645508\n",
      "st\n",
      "1.675158977508545\n",
      "\n",
      "=====REAL NAMES=====\n",
      "SaamirE\n",
      "16.701412200927734\n",
      "SaaronE\n",
      "12.982748031616211\n",
      "SabbeyE\n",
      "13.205779075622559\n",
      "SabbieE\n",
      "14.282242774963379\n",
      "SabbotE\n",
      "15.989633560180664\n"
     ]
    }
   ],
   "source": [
    "def generate_name_bigram(counts: torch.TensorType):\n",
    "    name = [alphabet[\"S\"]]\n",
    "    while name[-1] != alphabet[\"E\"]:\n",
    "        row = counts[name[-1], :]\n",
    "        next_letter_idx = torch.multinomial(row, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "    return name[1:-1]\n",
    "\n",
    "def display_name(name: list[int]):\n",
    "    return \"\".join([rev_alphabet[idx] for idx in name])\n",
    "\n",
    "for i in range(5):\n",
    "    name = generate_name_bigram(counts)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))\n",
    "print(\"\\n=====REAL NAMES=====\")\n",
    "for i in range(5):\n",
    "    print(display_name(names[i].tolist()))\n",
    "    print(name_log_prob(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qphvmfosjfmg\n",
      "43.074161529541016\n",
      "ys\n",
      "3.807882785797119\n",
      "nmrsuzgyx jfypfvezn\n",
      "nan\n",
      "sh abqrqprnkirrzgvw\n",
      "nan\n",
      "npcrbwybjiebo ttxge\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Figure out the counts using NN\n",
    "from torch import nn\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, hidden_size) # Would use embedding if you had a larger vocab\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=0) # Why dim=1?\n",
    "\n",
    "    def forward(self, input: torch.TensorType) -> torch.TensorType:\n",
    "        output = self.linear(input)\n",
    "        return self.softmax(output)\n",
    "\n",
    "def generate_name_nn(model: BigramModel):\n",
    "    name = [alphabet[\"S\"]]\n",
    "    i = 0\n",
    "    while name[-1] != alphabet[\"E\"] and i < 20:\n",
    "        # One-hot encoding of last letter\n",
    "        input = torch.zeros(model.vocab_size)\n",
    "        input[name[-1]] += 1\n",
    "        # Outputs probabilities row\n",
    "        probs = model.forward(input)\n",
    "        # Sample next letter\n",
    "        next_letter_idx = torch.multinomial(probs, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "        i += 1\n",
    "\n",
    "    return name[1:-1]\n",
    "\n",
    "model = BigramModel(len(alphabet))\n",
    "for i in range(5):\n",
    "    name = generate_name_nn(model)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for name in names:\n",
    "    # Reset gradient\n",
    "    model.zero_grad()\n",
    "    # Compute loss\n",
    "    loss = 0\n",
    "    for bigram in zip(name, name[1:]):\n",
    "        input = torch.zeros(model.vocab_size)\n",
    "        input[bigram[0]] += 1\n",
    "        probs = model.forward(input)\n",
    "        loss += -torch.log(probs[bigram[1]]) # Negative log likelihood loss\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    # Update parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ajstedn\n",
      "19.755199432373047\n",
      "\n",
      "-0.0\n",
      "almv\n",
      "10.270076751708984\n",
      "saily\n",
      "10.595558166503906\n",
      "shil\n",
      "6.770891189575195\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    name = generate_name_nn(model)\n",
    "    print(display_name(name))\n",
    "    print(name_log_prob(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Log Probs:\n",
      "Delay:  12.633674325942993\n",
      "Bigram:  14.3560506772995\n"
     ]
    }
   ],
   "source": [
    "# Should try doing a trigram model. There are probably many increasingly clever ways to do this, worth making a few up before googling the canonical solutions.\n",
    "# Keep a delayed bigram and use this along with the normal bigram (average the rows, then take multinomial sample)\n",
    "# Simply use a 3D tensor (fine for trigram, not for much more)\n",
    "# Make an embedding of the last n letters\n",
    "# Use a recurrent neural network\n",
    "# You should try to make your owv version of the attention mechanism from memory, or whatever you think the word \"attention\" SHOULD mean. Then see how this later compares to an actual implementation\n",
    "\n",
    "def create_delayed_bigram(delay: int):\n",
    "    counts = torch.zeros(len(alphabet), len(alphabet), dtype=torch.float)\n",
    "    for name in names:\n",
    "        for bigram in zip(name, name[delay:]):\n",
    "            counts[bigram[0], bigram[1]] += 1\n",
    "    counts = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return counts\n",
    "\n",
    "\n",
    "delay_counts = []\n",
    "max_delay = 3\n",
    "for i in range(1, max_delay + 1):\n",
    "    delay_counts.append(create_delayed_bigram(i))\n",
    "\n",
    "def generate_name_delay(delay_counts):\n",
    "    name = [alphabet[\"S\"]]\n",
    "    while name[-1] != alphabet[\"E\"]:\n",
    "        row = delay_counts[0][name[-1], :]\n",
    "        for i in range(1, len(delay_counts)):\n",
    "            if len(name) > i:\n",
    "                row += delay_counts[i][name[-(i + 1)], :]\n",
    "        next_letter_idx = torch.multinomial(row, 1).item()\n",
    "        name.append(next_letter_idx)\n",
    "    return name[1:-1]\n",
    "\n",
    "\n",
    "delay_sum = 0\n",
    "bigram_sum = 0\n",
    "for i in range(100):\n",
    "    name = generate_name_delay(delay_counts)\n",
    "    # print(display_name(name))\n",
    "    delay_sum += name_log_prob(name)\n",
    "    bigram_sum += name_log_prob(generate_name_bigram(counts))\n",
    "print(\"Average Log Probs:\")\n",
    "print(\"Delay: \", delay_sum / 100.0)\n",
    "print(\"Bigram: \", bigram_sum / 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Delay Log Prob:  22.67301644742489\n",
      "Average Bigram Log Prob:  16.118463459014894\n"
     ]
    }
   ],
   "source": [
    "# Delay is doing worse, but this is because we are checking it against the model created by bigram, which isn't fair\n",
    "# How do you fairly compare the two? How about finding the loss of all the names\n",
    "# in the original dataset.\n",
    "\n",
    "def delay_name_log_prob(name: list[int]):\n",
    "    total = 0\n",
    "    for i in range(len(delay_counts)):\n",
    "        pairs = list(zip(name, name[i + 1:]))\n",
    "        a = [el[0] for el in pairs]\n",
    "        b = [el[1] for el in pairs]\n",
    "        total += -torch.sum(torch.log(0.01 + delay_counts[i][a, b])).item()\n",
    "\n",
    "    return total\n",
    "\n",
    "delay_sum = 0\n",
    "bigram_sum = 0\n",
    "for i in range(100):\n",
    "    name = names[i]\n",
    "    delay_sum += delay_name_log_prob(name)\n",
    "    bigram_sum += name_log_prob(name)\n",
    "print(\"Average Delay Log Prob: \", delay_sum / 100.0)\n",
    "print(\"Average Bigram Log Prob: \", bigram_sum / 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like the delay is doing worse.\n",
    "# Next idea though is to try learning which of the delays to pay attention to in which situations\n",
    "# Perhaps you want to make a NN: previous n letters -> softmax over the delays\n",
    "# Then dot the delays with the output of this NN to get the row which you sample from\n",
    "\n",
    "class NateAttentionModel:\n",
    "    def __init__(self, vocab_size: int, max_delay: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_delay = max_delay\n",
    "        self.linear = nn.Linear(vocab_size * max_delay, max_delay)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, input: torch.TensorType) -> torch.TensorType:\n",
    "        output = self.linear(input)\n",
    "        return self.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something else you have to take into account is that if you are using a max_delay\n",
    "# of n, you should be prefixing your names with n S's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with RNN - This is a prebuilt model that you can test against. But should build your own.\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.embedding = nn.Embedding(vocab_size, hidden_size) # Would use embedding if you had a larger vocab\n",
    "        self.rnn = nn.RNN(vocab_size, vocab_size)\n",
    "        self.linear = nn.Linear(vocab_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, input: torch.TensorType, hidden: torch.TensorType) -> torch.TensorType:\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.linear(output)\n",
    "        return self.softmax(output), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
