{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this notebook is to determine what factors affect the stability of a model's kernel.\n",
    "# First, compare 1) when you train a model to put some vector in its kernel, vs. 2) when you search for a vector which is in the kernel of an untrained network\n",
    "# Depth, width, sparsity\n",
    "# Are more highly trained models more stable?\n",
    "# Are certain members of the kernel more stable than others?\n",
    "# Obviously the stability will depend on output functions like softmax. How do activation functions affect stability?\n",
    "# How does accuracy trade of with stability (add a stability term to the loss function)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def epsilon_stability(model: nn.Module, k: torch.Tensor, epsilon: torch.Tensor):\n",
    "    assert k.shape == epsilon.shape, \"kernel and epsilon must have same shape\"\n",
    "    return torch.norm(model(k + epsilon)).item()\n",
    "\n",
    "# Want a way to measure the stability of each of the inputs.\n",
    "def neuron_stability(model: nn.Module, k: torch.Tensor, epsilon: float, idx: int):\n",
    "    assert idx <= k.shape[0], \"idx must be less than size of vector k\"\n",
    "    epsilon_vec = torch.ones(k.shape)\n",
    "    epsilon_vec[idx] = epsilon\n",
    "    return epsilon_stability(model, k, epsilon_vec)\n",
    "\n",
    "# Average over the stability of each neuron\n",
    "def avg_neuron_stability(model: nn.Module, k: torch.Tensor, epsilon: float):\n",
    "    stabilities = [neuron_stability(model, k, epsilon, idx) for idx in range(k.shape[0])]\n",
    "    return sum(stabilities) / k.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self, inputs: int, outputs: int):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.linear = nn.Linear(inputs, outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.linear.reset_parameters()\n",
    "\n",
    "test_model = TestModel(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_only_kernel(model: nn.Module, k: torch.Tensor, epochs: int):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.norm(model(k))\n",
    "        # print(\"epoch \" + str(epoch) + \": \" + str(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "class LiterallyJustAVector(nn.Module):\n",
    "    def __init__(self, length: int):\n",
    "        super().__init__()\n",
    "        self.x = nn.Parameter(torch.randn(length))\n",
    "        self.register_parameter(\"kernel\", param=self.x)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.x\n",
    "\n",
    "\n",
    "def search_for_kernel(model: nn.Module, epochs: int):\n",
    "    kernel_model = LiterallyJustAVector(model.inputs)\n",
    "    optimizer = torch.optim.Adam(kernel_model.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        kernel = kernel_model.forward()\n",
    "        loss = torch.norm(model(kernel))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return kernel_model.forward()\n",
    "\n",
    "def train_to_kernel_with_noise(model: nn.Module, k: torch.Tensor, epochs: int, x: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Training a model on only the kernel is likely to come up with trivial solutions, so trying here to also add in other data.\n",
    "    Later want to use actual data instead of noise. Curious how the randomness/distribution of data affects the kernel, also how high-dimensional is is.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    kernel = torch.randn(test_model.inputs) # How does training to kernels of different distributions change stability?\n",
    "    train_to_only_kernel(test_model, kernel, i * 10)\n",
    "    print(str(10*i) + \" epochs\")\n",
    "    print(\"Neuron stabilities: \", avg_neuron_stability(test_model, kernel, 0.01))\n",
    "    print(\"model(kernel) = \", torch.norm(test_model(kernel)).item())\n",
    "    print(\"\\n\")\n",
    "    test_model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel stability:  1.2401344776153564\n",
      "SINGLE STABILITY:  2356.34521484375\n",
      "model(kernel) =  0.0014170118374750018\n",
      "Random stability:  1.4782097220420838\n",
      "average of model(randn) =  0.7609876424074173\n"
     ]
    }
   ],
   "source": [
    "kernel = search_for_kernel(test_model, 200)\n",
    "# Why is the stability the same every time I train?? Given some epsilon and this model, the kernel stability is the same.\n",
    "print(\"Kernel stability: \", avg_neuron_stability(test_model, kernel, 1.0))\n",
    "print(\"SINGLE STABILITY: \", sum([neuron_stability(test_model, kernel, 1000, i) for i in range(kernel.shape[0])]))\n",
    "\n",
    "print(\"model(kernel) = \", torch.norm(test_model(kernel)).item())\n",
    "total_stability = 0\n",
    "total_norm = 0\n",
    "samples = 10\n",
    "for i in range(samples):\n",
    "    sample = torch.randn(test_model.inputs)\n",
    "    total_stability += avg_neuron_stability(test_model, sample, 1.0)\n",
    "    total_norm += torch.norm(test_model(sample)).item()\n",
    "print(\"Random stability: \", total_stability / samples)\n",
    "print(\"average of model(randn) = \", total_norm / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
